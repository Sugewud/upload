{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "prompt = [\"a photograph of an astronaut riding a horse\"]\n",
    "height = 512                        # default height of Stable Diffusion\n",
    "width = 512                         # default width of Stable Diffusion\n",
    "num_inference_steps = 100           # Number of denoising steps\n",
    "guidance_scale = 7.5                # Scale for classifier-free guidance\n",
    "torch.manual_seed(0)                # 看情况是否固定随机种子\n",
    "batch_size = len(prompt)\n",
    "device = 'cuda'\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\"pretrain_model/stable-diffusion-v1-4\", subfolder=\"vae\", torch_dtype=torch.float16).to(device)\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "scheduler.set_timesteps(num_inference_steps)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"pretrain_model/stable-diffusion-v1-4\", subfolder=\"tokenizer\", torch_dtype=torch.float16)\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"pretrain_model/stable-diffusion-v1-4\", subfolder=\"text_encoder\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "def text_encode(prompts, maxlen=None):\n",
    "    '''\n",
    "    A function to take a texual promt and convert it into embeddings\n",
    "    '''\n",
    "    if maxlen is None: maxlen = tokenizer.model_max_length\n",
    "    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\") \n",
    "    return text_encoder(inp.input_ids.to(\"cuda\"))[0].half()\n",
    "\n",
    "def load_image(p):\n",
    "    '''\n",
    "    Function to load images from a defined path\n",
    "    '''\n",
    "    return Image.open(p).convert('RGB').resize((512,512))\n",
    "\n",
    "def pil_to_latents(image):\n",
    "    '''\n",
    "    Function to convert image to latents\n",
    "    '''\n",
    "    init_image = transforms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0\n",
    "    init_image = init_image.to(device=\"cuda\", dtype=torch.float16) \n",
    "    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215\n",
    "    return init_latent_dist\n",
    "\n",
    "strength = 0.8 #控制一开始给图像加噪的程度\n",
    "\n",
    "image = load_image('image/2.png')\n",
    "prompt = [\"Wolf howling at the moon, photorealistic 4K\"]\n",
    "\n",
    "text_embeddings = text_encode(prompt) \n",
    "uncond_embeddings = text_encode([\"\"] * batch_size, text_embeddings.shape[1])\n",
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "init_latents = pil_to_latents(image)\n",
    "\n",
    "# Figuring initial time step based on strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([999.0000, 988.9091, 978.8182, 968.7273, 958.6364, 948.5455, 938.4545,\n",
      "        928.3636, 918.2727, 908.1818, 898.0909, 888.0000, 877.9091, 867.8182,\n",
      "        857.7273, 847.6364, 837.5455, 827.4545, 817.3636, 807.2727, 797.1818,\n",
      "        787.0909, 777.0000, 766.9091, 756.8182, 746.7273, 736.6364, 726.5455,\n",
      "        716.4545, 706.3636, 696.2727, 686.1818, 676.0909, 666.0000, 655.9091,\n",
      "        645.8182, 635.7273, 625.6364, 615.5455, 605.4545, 595.3636, 585.2727,\n",
      "        575.1818, 565.0909, 555.0000, 544.9091, 534.8182, 524.7273, 514.6364,\n",
      "        504.5454, 494.4546, 484.3636, 474.2727, 464.1818, 454.0909, 444.0000,\n",
      "        433.9091, 423.8182, 413.7273, 403.6364, 393.5454, 383.4546, 373.3636,\n",
      "        363.2727, 353.1818, 343.0909, 333.0000, 322.9091, 312.8182, 302.7273,\n",
      "        292.6364, 282.5454, 272.4546, 262.3636, 252.2727, 242.1818, 232.0909,\n",
      "        222.0000, 211.9091, 201.8182, 191.7273, 181.6364, 171.5455, 161.4545,\n",
      "        151.3636, 141.2727, 131.1818, 121.0909, 111.0000, 100.9091,  90.8182,\n",
      "         80.7273,  70.6364,  60.5455,  50.4545,  40.3636,  30.2727,  20.1818,\n",
      "         10.0909,   0.0000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "init_timestep = int(num_inference_steps * strength)  #80\n",
    "timesteps = scheduler.timesteps[-init_timestep] #tensor(797.1818)\n",
    "timesteps = torch.tensor([timesteps], device=device)\n",
    "\n",
    "print(scheduler.timesteps)\n",
    "\n",
    "# Adding noise to the latents \n",
    "noise = torch.randn(init_latents.shape, device=device, dtype=init_latents.dtype)\n",
    "init_latents = scheduler.add_noise(init_latents, noise, timesteps) #把x0加噪到x797\n",
    "latents = init_latents\n",
    "\n",
    "# Computing the timestep to start the diffusion loop\n",
    "t_start = max(num_inference_steps - init_timestep, 0) #20\n",
    "timesteps = scheduler.timesteps[t_start:].to(device)  #从步数797开始去噪\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "99c97bf3c9d3c81a739cd20f2d7c2881a2e0795adf21ca2718ef10011e9d8acb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.18 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
